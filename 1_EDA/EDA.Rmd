---
title: "Exploratory data analysis"
author: "Zijun Lu"
date: "12/6/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load in the data and libraries

```{r}
library(tidyverse)
library(data.table)
library(mice)

churn=fread("churn.csv",stringsAsFactors = FALSE)
```

```{r}
glimpse(churn)
```

Let us look at the data to see if there is any missing value from the data.

```{r}
source("https://raw.githubusercontent.com/edwardcooper/mlmodel_select/master/miss_pct_plot.R")
miss_pct_plot(churn)
```

It seems that there is a small amount of missing value is TotalCharges. We could use random forest to impute the data. 

```{r}
library(mice)
churn_impute1=churn%>%mice(method = "rf",m=1,seed=7)%>%complete(1)
```



```{r,fig.width=12,fig.height=9}
library(tabplot)
tableplot(churn_impute1,sortCol = Churn)
```


It seems that customerID does not have a strong relationship with the target variable, churn.

All the other variables seem to have some relationship with the target variable. 

In order to feed the data to machine learning models, we will need to do some one-hot-encoding. (Doing one-hot-encoding is not common in R but is common in python)


```{r}
churn_impute1[,"customerID"]=NULL
library(caret)
churn_impute1_dummy=dummyVars("Churn~.",data=churn_impute1)%>%predict(newdata=churn_impute1)%>%data.frame()
churn_impute1_dummy=cbind(Churn=churn$Churn,churn_impute1_dummy)%>%as.data.frame()
```


Let us see if there is any correlation between features. 

```{r, fig.width=11,fig.height=11}
cor(dummyVars("~.",data=churn_impute1)%>%predict(newdata=churn_impute1)%>%data.frame())%>%corrplot()
```

Last but not the least, we need to see if there is class imbalance in the target variable.

```{r}
table(churn_impute1_dummy$Churn)
```



Thus, it is clear that we need to deal with the clas imbalance in machine learning model training part. 


